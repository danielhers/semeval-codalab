<h3>Evaluation Criteria</h3>
<h3 id="submission-conditions.">Submission conditions</h3>
<p>Participant systems in the task were evaluated in four settings:</p>
<ol>
    <li>
        <p>English in-domain setting, using the Wiki corpus.</p>
    </li>
    <li>
        <p>English out-of-domain setting, using the Wiki corpus as training and development data, and 20K Leagues as
            test data.</p>
    </li>
    <li>
        <p>German in-domain setting, using the 20K Leagues corpus.</p>
    </li>
    <li>
        <p>French setting with no training data (except trial data), using the 20K Leagues corpus as development and
            test data.</p>
    </li>
</ol>
<p>In order to allow both even ground comparison between systems and using hitherto untried resources, we held both an
    <strong>open</strong> and a <strong>closed</strong> track for submissions in the English and German settings. Closed
    track submissions are only allowed to use the gold-standard UCCA annotation distributed for the task in the target
    language, and are limited in their use of additional resources. Concretely, the additional data they are allowed to
    use is only that used by TUPA, which consists of automatic named entity annotations provided by spaCy<a id="fnref1"
                                                                                                            class="footnote-ref"
                                                                                                            href="#fn1"><sup>1</sup></a>,
    and automatic POS tags and syntactic dependency relations provided by UDPipe.<a id="fnref2" class="footnote-ref"
                                                                                    href="#fn2"><sup>2</sup></a>&nbsp;In
    addition, the closed track allows the use of word embeddings provided by fastText<a id="fnref2" class="footnote-ref"
                                                                                        href="#fn3"><sup>3</sup> </a>
    for all languages.</p>
<p>Systems in the open track, on the other hand, are allowed to use any additional resource, such as UCCA annotation in
    other languages, dictionaries or datasets for other tasks, provided that they make sure not to use any additional
    gold standard annotation over the same text used in the UCCA corpora.<a id="fnref3" class="footnote-ref"
                                                                            href="#fn4"><sup>4</sup> </a> In both
    tracks, we require that submitted systems will not be trained on the development data. Development data can be used
    for tuning. Due to the absence of an established pilot study for French, we only hold an open track for this
    setting. Training for French is allowed on the trial data (15 sentences).</p>
<p>The four settings and two tracks result in a total of<strong> 7 competitions</strong>, where a team may participate
    in anywhere between 1 and 7 of them. We encourage submissions in each track to use their systems to produce results
    in all settings. In addition, we encourage closed-track submissions to also submit to the open track.</p>
<h3>Formats</h3>
<p>For ease of submission in addition to the UCCA xml files <a
        href="http://alt.qcri.org/semeval2015/task18/index.php?id=data-and-tools">sdp</a>, <a
        href="http://universaldependencies.org/format.html">conllu</a>, <a
        href="http://ufal.mff.cuni.cz/conll2009-st/task-description.html">conll</a>, <a
        href="http://www.coli.uni-saarland.de/~thorsten/publications/Brants-CLAUS98.pdf">export</a> and <a
        href="https://github.com/amrisi/amr-guidelines/blob/master/amr.md">amr</a> formats are allowed too, such
    submissions will be automatically converted to UCCA using <a
            href="https://github.com/huji-nlp/semstr/blob/master/semstr/convert.py">this script</a>.</p>
<p>To convert manually:</p>
<pre>pip install semstr <br/>python -m semstr.convert [filenames] -f [format] -o [out_dir]</pre>
<p>Note that while the NeGra <strong>export</strong> format preserves all the information in the UCCA graphs, conversion
    to the <strong>sdp</strong>, <strong>conllu</strong>, <strong>conll</strong> and <strong>amr</strong> formats is
    lossy, due to the bilexical dependency structure (and due to reentrancies in AMR not being separated to primary and
    remote). Below are the labeled scores of converting the English Wiki corpus to these formats and back to the
    standard format:</p>
<table border="0">
    <tbody>
    <tr>
        <td style="width: 2cm;" scope="col">&nbsp;</td>
        <td style="width: 2cm;" scope="col" colspan="3"><strong>Primary</strong></td>
        <td style="width: 2cm;" scope="col" colspan="3"><strong>Remote</strong></td>
    </tr>
    <tr>
        <td style="width: 2cm;" scope="col">&nbsp;</td>
        <td style="width: 2cm;" scope="col"><strong>LP</strong></td>
        <td style="width: 2cm;" scope="col"><strong>LR</strong></td>
        <td style="width: 2cm;" scope="col"><strong>LF</strong></td>
        <td style="width: 2cm;" scope="col"><strong>LP</strong></td>
        <td style="width: 2cm;" scope="col"><strong>LR</strong></td>
        <td style="width: 2cm;" scope="col"><strong>LF</strong></td>
    </tr>
    <tr>
        <td style="width: 2cm;" scope="col">sdp</td>
        <td style="width: 2cm;" scope="col">95.7</td>
        <td style="width: 2cm;" scope="col">92.8</td>
        <td style="width: 2cm;" scope="col">94.2</td>
        <td style="width: 2cm;" scope="col">95</td>
        <td style="width: 2cm;" scope="col">47.9</td>
        <td style="width: 2cm;" scope="col">63.7</td>
    </tr>
    <tr>
        <td style="width: 2cm;" scope="col">conllu</td>
        <td style="width: 2cm;" scope="col">90.4</td>
        <td style="width: 2cm;" scope="col">89</td>
        <td style="width: 2cm;" scope="col">89.7</td>
        <td style="width: 2cm;" scope="col">99.9</td>
        <td style="width: 2cm;" scope="col">47.7</td>
        <td style="width: 2cm;" scope="col">64.6</td>
    </tr>
    <tr>
        <td style="width: 2cm;" scope="col">conll</td>
        <td style="width: 2cm;" scope="col">93.2</td>
        <td style="width: 2cm;" scope="col">92</td>
        <td style="width: 2cm;" scope="col">92.6</td>
        <td style="width: 2cm;" scope="col">95.6</td>
        <td style="width: 2cm;" scope="col">48.2</td>
        <td style="width: 2cm;" scope="col">64.1</td>
    </tr>
    <tr>
        <td style="width: 2cm;" scope="col">amr</td>
        <td style="width: 2cm;" scope="col">97.4</td>
        <td style="width: 2cm;" scope="col">97.4</td>
        <td style="width: 2cm;" scope="col">97.4</td>
        <td style="width: 2cm;" scope="col">88.8</td>
        <td style="width: 2cm;" scope="col">88.7</td>
        <td style="width: 2cm;" scope="col">88.8</td>
    </tr>
    </tbody>
</table>
<p>&nbsp;</p>
<h3 id="scoring.">Scoring</h3>
<p>In order to evaluate how similar an output UCCA structure is to a gold UCCA graph, we use <span> <em>DAG <span
        class="math inline">F<sub>1</sub></span>-score</em> </span>. Formally, over two UCCA annotations <span
        class="math inline">G<sub>1</sub></span>&nbsp;and <span class="math inline">G<sub>2</sub></span> that share
    their set of leaves (tokens) <span class="math inline">W</span>&nbsp;and for a node <span
            class="math inline">v</span>&nbsp;in <span class="math inline">G<sub>1</sub></span>&nbsp;or&nbsp;<span
            class="math inline">G<sub>2</sub></span>&nbsp;, define its yield <span class="math inline">(yield(v) subset or equal W)</span>
    as its set of leaf descendants. Define a pair of edges <span class="math inline">((v<sub>1</sub>,u<sub>1</sub>) in G<sub>1</sub>)</span>
    and <span class="math inline">((v<sub>2</sub>,u<sub>2</sub>) in G<sub>2</sub>)</span> to be matching if <span
            class="math inline">(yield(u<sub>1</sub>) = yield(u<sub>2</sub>))</span> and they have the same label.
    Labeled Precision and Recall are defined by dividing the number of matching edges in <span
            class="math inline">G<sub>1</sub></span>&nbsp;and&nbsp;<span class="math inline">G<sub>2</sub></span>&nbsp;&nbsp;by
    <span class="math inline">|E<sub>1</sub>|</span>&nbsp;and <span class="math inline">|E<sub>2</sub>|</span>&nbsp;respectively.
    <span> <em>DAG <span class="math inline">F<sub>1</sub></span>-score</em> </span> is their harmonic mean.&nbsp; We
    will report Precision, Recall and F1 scores both for <strong>primary</strong> and <strong>remote</strong> edges. For
    the sake of this task's evaluation implicit units are disregarded and do not count for the evaluation. Also, the
    measures are indifferent to the position of the Function category.</p>
<p>The Center (C) category is disregarded by the evaluation in the two following cases:</p>
<p>1. If the unique child v of a node u is annotated as C, then v is disregarded. So in this case, if v is a leaf, u
    will be considered as a leaf instead of v and if v is not a leaf, the child nodes of v will be considered as the
    child nodes of u.</p>
<p>2. If v is a unique center in a unit u (i.e. the other children of u are not annotated as centers), and w is a unique
    center in v, then v is disregarded. That is, the child nodes of v (including w) will be considered as the child
    nodes of u.</p>
<p>Normalization will be automatically run before the evaluation using <a
        href="https://github.com/danielhers/ucca/blob/master/ucca/normalization.py">this script</a>.</p>
<p>For each of the seven competitions, we will report winning systems according to the <strong>Primary F1-score</strong>
    and according to the <strong>Remote F1-score</strong>.</p>
<p>For a more fine-grained evaluation, Precision, Recall and F1 scores of specific category (edge labels) will also be
    reported. UCCA labels can be divided into categories that correspond to Scene elements (States, Processes,
    Participants, Adverbials), non-Scene elements (Elaborators, Connectors, Centers), and inter-Scene Linkage (Parallel
    Scenes, Linkage, Ground). We will report performance for each of these sets separately, leaving out Function and
    Relator units that do not belong to any particular model.</p>
<p>&nbsp;</p>
<p>To evaluate manually:</p>
<pre>pip install semstr <br/>python -m semstr.evaluate <code>[predicated_file_or_dir] [reference_file_or_dir]</code></pre>
<p>&nbsp;</p>
<p>&nbsp;</p>
<hr/>
<ol>
    <li id="fn1">
        <p><a class="uri" href="http://spacy.io">http://spacy.io</a>. We use spaCy 2.0.12 with the&nbsp;en_core_web_lg,&nbsp;fr_core_news_md
            and&nbsp;de_core_news_sm models. <a class="footnote-back" href="#fnref1">↩</a></p>
    </li>
    <li><a href="http://ufal.mff.cuni.cz/udpipe">http://ufal.mff.cuni.cz/udpipe</a>. We use UDPipe 1.2 with the <a
            href="http://ufal.mff.cuni.cz/~zeman/soubory/ud-2.2-conll18-baseline-models.tar.xz">CoNLL 2018 baseline
        models</a>&nbsp;traind on English-EWT, French-GSD and German-GSD.&nbsp;<a class="footnote-back"
                                                                                  href="#fnref2">↩</a></li>
    <li id="fn2">
        <p><a class="uri" href="http://fasttext.cc">http://fasttext.cc</a> <a class="footnote-back" href="#fnref3">↩</a>
        </p>
    </li>
    <li id="fn3">
        <p>We are not aware of any such annotation, but include this restriction for completeness.<a
                class="footnote-back" href="#fnref3">↩</a></p>
    </li>
</ol>